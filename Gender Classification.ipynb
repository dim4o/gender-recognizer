{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gender Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "The aim of the article is to choose the best gender classification model between Logistic Regresson, Decision Tree Claifier, SVM and kNH algorithms. The particular thing about this is that we have relatively small data about famous public figures. We will also see how well the chosen model works for normal people like us. I suspect that the best model will be SVM, but let's see what happens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of content\n",
    "- [Introduction](#Introduction)\n",
    "- [Data processing](#Data-processing)\n",
    "    - [How to get the image dataset?](#How-to-get-the-image-dataset?)\n",
    "    - [Image data description](#Image-data-description)\n",
    "    - [Reducing the examples and balancing the data](#Reducing-the-examples-and-balancing-the-data)\n",
    "    - [Separating the data](#Separating-the-data)\n",
    "    - [Feature extraction](#Feature-extraction)\n",
    "- [Feature selection](#Feature-selection)\n",
    "    - [Data normalization](#Data-normalization)\n",
    "    - [Chose the best features](#Chose-the-best-features)\n",
    "- [Model selection](#Model-selection)\n",
    "    - [Logistic regression](#Logistic-Regression)\n",
    "        - [Basic Logistic Regrassion](#Basic-logistic-regrassion)\n",
    "            - [Tune the Parameters of the Basic Linear Regression](Tune-the-Parameters-of-the-Basic-Linear-Regression)\n",
    "        -[Polynomial Logistic Regression](Polynomial-Logistic-Regression)\n",
    "        - tune the params with grid search CV\n",
    "    - [Trees](#Trees)\n",
    "        - [Decision tree](#Decision-tree)\n",
    "            - try basic decision tree\n",
    "            - tune the params with grid search CV\n",
    "        - [Random forest clasifier](#Random-Forest-Clasifier)\n",
    "            - try basic random forest\n",
    "            - tune the params with grid search CV\n",
    "        - [Adaptive Boost](#Adaptive-Boost)\n",
    "            - try basic Adaptive Boost\n",
    "            - tune the params with grid search CV\n",
    "        - compare results and get the best score\n",
    "    - [kNN](#kNN)\n",
    "        - [Basic kNN Clasifier](#Basic kNN Clasifier)\n",
    "        - tune the params with the grid search CV\n",
    "    - [SVM](#SVM)\n",
    "        - [Linear SVM](#Linear-SVM)\n",
    "            - try a linear SVM\n",
    "            - tune the linear SVM params with grid search CV\n",
    "        - [Gaussian SVM](#Gaussian-SVM)\n",
    "            - try Gaussian SVM\n",
    "            - tune the Gaussian SVM params with grid search CV\n",
    "        - [Select the best SVM Model](Select-the-best-SVM Model)\n",
    "   - [Select the best model](#Select-the-best-model)\n",
    "        - print ROC curve\n",
    "        - print learning curve\n",
    "- [Conclusion](#Conclusion)\n",
    "- [Demo](#Demo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I recommend you to install the `opencv` through `pip` because there is well known issues with the `conda` distribution - especially for Linux. The `opencv` library is used for the demo because of easy work with camera and haar faces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: opencv-python in c:\\python27\\lib\\site-packages\n",
      "Requirement already satisfied: numpy>=1.11.1 in c:\\python27\\lib\\site-packages (from opencv-python)\n"
     ]
    }
   ],
   "source": [
    "#!conda remove --yes opencv\n",
    "!pip install opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import urllib\n",
    "import tarfile\n",
    "import re\n",
    "from shutil import copyfile\n",
    "import random\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import RFECV, SelectKBest, SelectPercentile\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data processing\n",
    "### How to get the image dataset?\n",
    "If you want to download end extract the image dataset you can run the cell bellow. It will automate the downloading and extracting the image data. Note that this is not very fact process. Alternatively you can download the image data from [here](http://vis-www.cs.umass.edu/lfw/lfw-funneled.tgz) and extract the `lfw-funneled.tar` file to `./faces_data/original_data/lfw-funneled` folder. On Windows for example you can use `7zip` extractor. Otherwise you can skip this part and downalod the generated `.csv` directly on the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downloads the  data\n",
    "print(\"Downloading image data...\")\n",
    "response = urllib.urlretrieve(\"http://vis-www.cs.umass.edu/lfw/lfw-funneled.tgz\")\n",
    "tar_url = response[0]\n",
    "print(\"Image data downalod location: '{}'\".format(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracts the downloaded data to './faces_data/original_data/lfw-funneled' directory\n",
    "def extract(tar_url, extract_path=\".\"):\n",
    "    print(\"Source directory: '{}'\".format(tar_url))\n",
    "    print(\"Extracting to: '{}'\".format(extract_path))\n",
    "    tar = tarfile.open(tar_url, 'r')\n",
    "    for item in tar:\n",
    "        tar.extract(item, extract_path)\n",
    "        if item.name.find(\".tgz\") != -1 or item.name.find(\".tar\") != -1:\n",
    "            extract(item.name, \"./\" + item.name[:item.name.rfind('/')])         \n",
    "            \n",
    "extract(tar_url, \"./faces_data/original_data\")\n",
    "print 'Done.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removes the downlaoded *.tar file\n",
    "try:\n",
    "    print(\"Removing '{}' ...\".format(tar_url))\n",
    "    os.remove(tar_url)\n",
    "except Exception as ex:\n",
    "    print(ex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image data description\n",
    "We have datacet with `~5700` persons. Each person is assosiated with a folders with several pictires. The all pictures are `~13200`. The folders are not labeled but we have files with the female names and male names, so we can label each image easy. Each picture has dimensions `250x250px`. \n",
    "`./faces_data/original_data/female_names.txt`  \n",
    "`./faces_data/original_data/male_names.txt`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will defice some constant. We will use `2900` samples for the females and the same amount for the males. The base image size is `40x40` the lables are: `female -> 0` and `male -> 1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUMBER_OF_SAMPLES = 2900\n",
    "RESIZE_SHAPE = (40, 40) # (64, 64)\n",
    "FEMALE_CLASS = 0\n",
    "MALE_CLASS = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make some calculations and data visualization by labels and plot them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f1c62feeb00>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Females: 2966\n",
      "Males: 10268\n"
     ]
    }
   ],
   "source": [
    "female_names_data = pd.read_csv('./faces_data/original_data/female_names.txt', header=None, sep='\\n')\n",
    "male_names_data = pd.read_csv('./faces_data/original_data/male_names.txt', header=None, sep='\\n')\n",
    "\n",
    "female_count = len(female_names_data)\n",
    "male_count = len(male_names_data)\n",
    "\n",
    "# plot diagram\n",
    "fig, ax = plt.subplots()\n",
    "colors = ['pink', 'blue']\n",
    "rectangles = ax.bar((1, 2), (female_count, male_count), color=colors, width=0.8)\n",
    "ax.xaxis.set_visible(False)\n",
    "ax.legend((rectangles[0], rectangles[1]), ('Females', 'Males'))\n",
    "plt.show()\n",
    "\n",
    "print(\"Females: {}\".format(female_count))\n",
    "print(\"Males: {}\".format(male_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the graph we can see that we have high unbalanced data. Furthermore we have we have a relatively large number of exmples and a huge number number of features `250*250=62500` and each feature has 3 component because the picture is colorful. With the components the features are `187500`! For this reason, I will make some suggestions and simplifications so that I can train my machine learning model on my processor for some tolerable period of time:\n",
    "- I can get rid of the unbalanced data and the large number of examples by making the women/men ratio ~50/50\n",
    "- I can reduce the huge number of features by making grayscale image and apply some image transformations. \n",
    "- In addition later I can use a feature selection algorithm to reduce the number of features.\n",
    "- I can perform data normalization because some algorihms works better with normalized data\n",
    "\n",
    "So there is something to be done. Let's get to work!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO:\n",
    "In owr case we have a small set of traning examples $N \\sim 1000$ and relatively large large set of features $M \\sim 3000$ i.e. $N < M$. In this case it is assumed that Ð° logistic regression or SVM without a kernel(linear SVM) will perform well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reducing the examples and balancing the data\n",
    "We have more men than women, so we can take all women and we can randomly get the same number of men:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Females: 2900\n",
      "Males: 2900\n"
     ]
    }
   ],
   "source": [
    "female_names = female_names_data.get_values()[0:, 0]\n",
    "male_names = male_names_data.get_values()[0:, 0]\n",
    "\n",
    "female_names = random.sample(set(female_names), NUMBER_OF_SAMPLES)\n",
    "male_names = random.sample(set(male_names), NUMBER_OF_SAMPLES)\n",
    "\n",
    "print(\"Females: {}\".format(len(female_names)))\n",
    "print(\"Males: {}\".format(len(male_names)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Separating the data\n",
    "Let's create two folders where we will hold the representatives of the relevant classes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_image_dir = \"./faces_data/original_data/lfw_funneled/\"\n",
    "directory_female = \"./faces_data/new_data/female/\"\n",
    "directory_male = \"./faces_data/new_data/male/\"\n",
    "if not os.path.exists(directory_female):\n",
    "    os.makedirs(directory_female)\n",
    "if not os.path.exists(directory_male):\n",
    "    os.makedirs(directory_male)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def separate_data(face_names_data, source, destination):\n",
    "    for image_name in face_names_data:\n",
    "        name = re.match(\"([A-Za-z_-]+)_(\\d+.jpg)\", image_name)\n",
    "        if name is None:\n",
    "            print(\"Name: {}\".format(image_name))\n",
    "        name = name.group(1)    \n",
    "        path = source_image_dir + name + \"/\" + image_name\n",
    "#         if os.path.exists:\n",
    "#             print(path)\n",
    "        #print(path)\n",
    "        copyfile(path, destination + image_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# separates the data\n",
    "separate_data(female_names, source_image_dir, directory_female)\n",
    "separate_data(male_names, source_image_dir, directory_male)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2900\n"
     ]
    }
   ],
   "source": [
    "# test print for the selected names\n",
    "female_image_names = os.listdir(\"./faces_data/new_data/female\")\n",
    "male_image_names = os.listdir(\"./faces_data/new_data/male\")\n",
    "print(len(female_image_names))\n",
    "print(len(male_image_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I'll extract the features from the image following this actions:\n",
    "1. Get the original image\n",
    "2. Convet the image to garyscale\n",
    "3. Define the region of interests. Use haar faces to detect a face (if exist) on the image. If there is more than one faces, I'll get the square with the larger area.\n",
    "4. Crop the square from the gray image\n",
    "5. Apply gaussian kernel over the cripped image to reduce the noise.\n",
    "6. Resize the image to 40x40px  \n",
    "\n",
    "| | | |\n",
    "|: - :|: - :|: - :|\n",
    "|![original image](./markdown_res/Amelia_Vega_0001.jpg) | ![gray image](./markdown_res/Amelia_Vega_0001_gray.bmp) | ![face](./markdown_res/Amelia_Vega_0001_face.bmp) |\n",
    "| ![cropped face](./markdown_res/Amelia_Vega_0001_cropped.bmp) | ![resized face](./markdown_res/Amelia_Vega_0001_kernel.bmp) | ![resized face](./markdown_res/Amelia_Vega_0001_resized.bmp) |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "gender_faces_data = None\n",
    "gender_faces_data = np.empty(shape=(0,RESIZE_SHAPE[0]*RESIZE_SHAPE[1]+1)) #64*64+1\n",
    "face_cascade = cv2.CascadeClassifier('./faces_data/face.xml')\n",
    "\n",
    "def extract_features(image_path, label):\n",
    "    \"\"\" Extracts features from a image\n",
    "    :param image_path: the path to the image\n",
    "    :param label: the label of the image\n",
    "    :return: extracted features from the image with the corresponding label as a row matrix\n",
    "    \"\"\"\n",
    "    image = cv2.imread(image_path)\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    row = None\n",
    "    faces = face_cascade.detectMultiScale(gray, 1.3, 5)\n",
    "    if faces is not ():\n",
    "        face = sorted(faces, key=lambda x: (x[2] * x[3]), reverse=True)[0]\n",
    "        x, y, width, height = face\n",
    "        face_gray = gray[y: y + height, x: x + width]\n",
    "        resized = cv2.resize(face_gray, dsize=RESIZE_SHAPE, interpolation=cv2.INTER_CUBIC)\n",
    "        row = np.append(resized.ravel(), label)\n",
    "\n",
    "    return row\n",
    "\n",
    "def append_data(prefix_path, label, gender_faces_data):\n",
    "    image_names = os.listdir(prefix_path)\n",
    "    for image_path in image_names:\n",
    "        # print(image_path)\n",
    "        curr_row = extract_features(prefix_path + image_path, label)\n",
    "        if curr_row is not None:\n",
    "            gender_faces_data = np.append(gender_faces_data, [curr_row], axis=0)\n",
    "        \n",
    "    return gender_faces_data  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'shap'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0mTraceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-fad07a25ee75>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# Append the male faces\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mgender_faces_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mappend_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'./faces_data/new_data/male/'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mMALE_CLASS\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgender_faces_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mmale_data_shape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgender_faces_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshap\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfemale_data_shape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'shap'"
     ]
    }
   ],
   "source": [
    "# Append the female faces\n",
    "gender_faces_data = append_data('./faces_data/new_data/female/', FEMALE_CLASS, gender_faces_data)\n",
    "female_data_shape = gender_faces_data.shape\n",
    "\n",
    "# Append the male faces\n",
    "gender_faces_data = append_data('./faces_data/new_data/male/', MALE_CLASS, gender_faces_data)\n",
    "male_data_shape = gender_faces_data.shape\n",
    "\n",
    "print(female_data_shape)\n",
    "print(male_data_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[126., 111.,  77., ..., 103.,  72.,   0.],\n",
       "       [  1.,   0.,   4., ...,  12.,  15.,   0.],\n",
       "       [ 15.,  70., 119., ...,  71.,  72.,   1.],\n",
       "       ...,\n",
       "       [ 53.,  60.,  60., ..., 127., 115.,   0.],\n",
       "       [138., 135., 132., ..., 161., 159.,   1.],\n",
       "       [162., 195., 183., ...,  91.,  79.,   1.]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Suffle the data\n",
    "np.random.shuffle(gender_faces_data)\n",
    "gender_faces_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data normalization\n",
    "It is better to normalize the data before trying diffrent feature selection algorihms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5481, 1600)\n",
      "(5481,)\n",
      "[0. 0. 1. ... 0. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "# get features\n",
    "features = gender_faces_data[:, :RESIZE_SHAPE[0]*RESIZE_SHAPE[1]]\n",
    "print(features.shape)\n",
    "\n",
    "# get labels\n",
    "labels = gender_faces_data[:, RESIZE_SHAPE[0]*RESIZE_SHAPE[1]:].ravel() # labels = gender_faces_data.as_matrix()[:, 50*50:].ravel()\n",
    "print(labels.shape)\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Females count: 2738\n",
      "males count: 2743\n",
      "{0.0: 2738, 1.0: 2743}\n"
     ]
    }
   ],
   "source": [
    "# scale the data with the StandardScaler algorithm\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(features)\n",
    "features = scaler.transform(features)\n",
    "features\n",
    "\n",
    "# calculates the final counts\n",
    "unique, count = np.unique(labels, return_counts=True)\n",
    "print(\"Females count: {}\".format(count[0]))\n",
    "print(\"Males count: {}\".format(count[1]))\n",
    "# out = np.histogram(labels, bins=labels)\n",
    "print(dict(zip(unique, count)))\n",
    "\n",
    "# TRAIN TEST SPLIT\n",
    "# features_train, features_test, labels_train, labels_test = train_test_split(\n",
    "#     features, labels, train_size=0.7, test_size=0.3, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chose the best features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pca = PCA(n_components=80)\n",
    "# pca.fit(features)\n",
    "# rediced_features = pca.transform(features)\n",
    "# rediced_features.shape\n",
    "\n",
    "# logistic_regression = LogisticRegression() # C=0.001\n",
    "# logistic_regression.fit(rediced_features, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5481, 1600)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reducing features to 80\n",
      "0.8464546402502607\n",
      "0.8285714285714286\n",
      "Reducing features to 140\n",
      "0.8597497393117831\n",
      "0.8340425531914893\n",
      "Reducing features to 200\n",
      "0.863660062565172\n",
      "0.8364741641337387\n",
      "Reducing features to 260\n",
      "0.8657455683003128\n",
      "0.8389057750759878\n",
      "Reducing features to 440\n",
      "0.8691345151199166\n",
      "0.8425531914893617\n",
      "Reducing features to 1000\n",
      "0.8706986444212722\n",
      "0.8425531914893617\n"
     ]
    }
   ],
   "source": [
    "features_counts = [80, 140, 200, 260, 440, 1000]\n",
    "test_scores = []\n",
    "train_scores = []\n",
    "for f_count in features_counts:\n",
    "    print(\"Reducing features to {}\".format(f_count))\n",
    "    pca = PCA(n_components=f_count)\n",
    "\n",
    "    pca.fit(features)\n",
    "    reduced_features = pca.transform(features)\n",
    "    \n",
    "    features_train, features_test, labels_train, labels_test = train_test_split(\n",
    "        reduced_features, labels, train_size=0.7, test_size=0.3, random_state=42)\n",
    "    #print(reduced_features.shape)\n",
    "    \n",
    "    #labels = labels.reshape((len(labels), 1)) # \n",
    "    #print(labels.shape)\n",
    "    \n",
    "    logistic_regression = LogisticRegression(C=0.001) # C=0.001\n",
    "    logistic_regression.fit(features_train, labels_train)\n",
    "    \n",
    "    test_score = logistic_regression.score(features_test, labels_test)\n",
    "    train_score = logistic_regression.score(features_train, labels_train)\n",
    "    \n",
    "    print(train_score)\n",
    "    print(test_score)\n",
    "    \n",
    "    test_scores.append(test_score)\n",
    "    train_scores.append(train_score)\n",
    "\n",
    "    \n",
    "features_train, features_test, labels_train, labels_test = train_test_split(\n",
    "    features, labels, train_size=0.7, test_size=0.3, random_state=42)\n",
    "\n",
    "logistic_regression = LogisticRegression() # C=0.001\n",
    "logistic_regression.fit(features_train, labels_train)\n",
    "\n",
    "\n",
    "features_counts.append(1600)\n",
    "test_scores.append(logistic_regression.score(features_test, labels_test))\n",
    "train_scores.append(logistic_regression.score(features_train, labels_train))\n",
    "    \n",
    "# pca = PCA(n_components=100)\n",
    "# pca.fit(features)\n",
    "# rediced_features = pca.transform(features)\n",
    "# rediced_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.8285714285714286, 0.8340425531914893, 0.8364741641337387, 0.8389057750759878, 0.8425531914893617, 0.8425531914893617, 0.7939209726443769]\n",
      "[0.8464546402502607, 0.8597497393117831, 0.863660062565172, 0.8657455683003128, 0.8691345151199166, 0.8706986444212722, 0.9919186652763295]\n"
     ]
    }
   ],
   "source": [
    "print(test_scores)\n",
    "print(train_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x2b2d8850>]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1bb696b0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(features_counts, test_scores)\n",
    "plt.plot(features_counts, train_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the data for the model is generated. The only thing left is to keep it in a file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(\"./genders_data_40x40_pca_200_large.csv\", gender_faces_data.astype(np.int), delimiter=\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train logistic regression model\n",
    "Try Lasso Regression with panalty $L1$ to reduce the number of the features. My intuition suggests that you can reduce the number of dimensions to get a simple model. For example, the features that are at the edges of the images should definitely not have any weight for recognizing the gender. But let's see if intuition lied to me."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5481, 1601)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>1591</th>\n",
       "      <th>1592</th>\n",
       "      <th>1593</th>\n",
       "      <th>1594</th>\n",
       "      <th>1595</th>\n",
       "      <th>1596</th>\n",
       "      <th>1597</th>\n",
       "      <th>1598</th>\n",
       "      <th>1599</th>\n",
       "      <th>1600</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>53.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>55.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>81.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>...</td>\n",
       "      <td>150.0</td>\n",
       "      <td>156.0</td>\n",
       "      <td>147.0</td>\n",
       "      <td>133.0</td>\n",
       "      <td>119.0</td>\n",
       "      <td>117.0</td>\n",
       "      <td>132.0</td>\n",
       "      <td>127.0</td>\n",
       "      <td>115.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>18.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>86.0</td>\n",
       "      <td>119.0</td>\n",
       "      <td>...</td>\n",
       "      <td>54.0</td>\n",
       "      <td>62.0</td>\n",
       "      <td>58.0</td>\n",
       "      <td>62.0</td>\n",
       "      <td>78.0</td>\n",
       "      <td>104.0</td>\n",
       "      <td>148.0</td>\n",
       "      <td>162.0</td>\n",
       "      <td>178.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>148.0</td>\n",
       "      <td>195.0</td>\n",
       "      <td>148.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>135.0</td>\n",
       "      <td>162.0</td>\n",
       "      <td>166.0</td>\n",
       "      <td>156.0</td>\n",
       "      <td>134.0</td>\n",
       "      <td>...</td>\n",
       "      <td>72.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>94.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>111.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>62.0</td>\n",
       "      <td>53.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>47.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>...</td>\n",
       "      <td>164.0</td>\n",
       "      <td>176.0</td>\n",
       "      <td>191.0</td>\n",
       "      <td>200.0</td>\n",
       "      <td>202.0</td>\n",
       "      <td>205.0</td>\n",
       "      <td>205.0</td>\n",
       "      <td>209.0</td>\n",
       "      <td>208.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>74.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>76.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>76.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>53.0</td>\n",
       "      <td>57.0</td>\n",
       "      <td>...</td>\n",
       "      <td>83.0</td>\n",
       "      <td>87.0</td>\n",
       "      <td>69.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>77.0</td>\n",
       "      <td>76.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã 1601 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    0      1      2     3     4      5      6      7      8      9     ...   \\\n",
       "0   53.0   60.0   60.0  43.0  55.0   38.0   34.0   50.0   81.0  101.0  ...    \n",
       "1   18.0    9.0    6.0   7.0   6.0    8.0    6.0   30.0   86.0  119.0  ...    \n",
       "2  148.0  195.0  148.0  74.0  67.0  135.0  162.0  166.0  156.0  134.0  ...    \n",
       "3   62.0   53.0   54.0  47.0  42.0   60.0   67.0   50.0   43.0   56.0  ...    \n",
       "4   74.0   74.0   75.0  76.0  73.0   74.0   76.0   68.0   53.0   57.0  ...    \n",
       "\n",
       "    1591   1592   1593   1594   1595   1596   1597   1598   1599  1600  \n",
       "0  150.0  156.0  147.0  133.0  119.0  117.0  132.0  127.0  115.0   0.0  \n",
       "1   54.0   62.0   58.0   62.0   78.0  104.0  148.0  162.0  178.0   1.0  \n",
       "2   72.0  101.0   94.0   90.0   72.0   54.0  111.0   85.0   92.0   1.0  \n",
       "3  164.0  176.0  191.0  200.0  202.0  205.0  205.0  209.0  208.0   0.0  \n",
       "4   83.0   87.0   69.0   73.0   73.0   73.0   77.0   76.0   74.0   1.0  \n",
       "\n",
       "[5 rows x 1601 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gender_faces_data = pd.read_csv('./genders_data_40x40_pca_200_large.csv', sep=',', header=None)\n",
    "\n",
    "print(gender_faces_data.shape)\n",
    "gender_faces_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5481, 1600)\n"
     ]
    }
   ],
   "source": [
    "features = gender_faces_data.as_matrix()[:, :RESIZE_SHAPE[0]*RESIZE_SHAPE[1]] #features = gender_faces_data[:, :50*50] # features = gender_faces_data.as_matrix()[:, :50*50]\n",
    "print(features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = gender_faces_data.as_matrix()[:, RESIZE_SHAPE[0]*RESIZE_SHAPE[1]:].ravel() # labels = gender_faces_data.as_matrix()[:, 50*50:].ravel()\n",
    "print(labels.shape)\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model selection\n",
    "### Logistic Regression Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Basic logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5481, 1600)\n",
      "(5481,)\n"
     ]
    }
   ],
   "source": [
    "print(features.shape)\n",
    "print(labels.shape)\n",
    "pca = SelectPercentile(percentile=100)\n",
    "pca.fit(features, labels)\n",
    "rediced_features = pca.transform(features)\n",
    "rediced_features.shape\n",
    "\n",
    "features_train, features_test, labels_train, labels_test = train_test_split(\n",
    "    rediced_features, labels, train_size=0.7, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_regression = LogisticRegression(C=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=0.001, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logistic_regression.fit(features_train, labels_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.841337386018237"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logistic_regression.score(features_test, labels_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8709593326381647"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logistic_regression.score(features_train, labels_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tune the Logistic Regression Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold = StratifiedKFold(n_splits=8) # train with 8\n",
    "params = {'C': [0.0001, 0.001, 0.01, 1, 10], 'penalty': ['l2', 'l1']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=StratifiedKFold(n_splits=8, random_state=None, shuffle=False),\n",
       "       error_score='raise',\n",
       "       estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False),\n",
       "       fit_params=None, iid=True, n_jobs=1,\n",
       "       param_grid={'penalty': ['l2', 'l1'], 'C': [0.0001, 0.001, 0.01, 1, 10]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search_logregr = GridSearchCV(\n",
    "    logistic_regression, param_grid=params, cv=kfold, return_train_score=True)\n",
    "grid_search_logregr.fit(features_train, labels_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 0.01, 'penalty': 'l2'}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search_logregr.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8428050052137643"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search_logregr.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8496732026143791"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(labels_test, grid_search_logregr.predict(features_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 0., 0., 1., 0., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0.,\n",
       "       1., 0., 1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1.,\n",
       "       0., 1., 1., 0., 0., 1., 0., 0., 1., 0., 1., 1., 1., 0., 0., 0., 0.,\n",
       "       0., 1., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 1., 0., 1., 0., 0.,\n",
       "       0., 1., 1., 0., 1., 1., 1., 1., 0., 0., 0., 0., 1., 0., 0., 0., 1.,\n",
       "       1., 0., 0., 1., 1., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_train[:100]\n",
    "#labels_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 0., 0., 1., 0., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0.,\n",
       "       1., 0., 1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1.,\n",
       "       0., 1., 1., 0., 0., 1., 0., 0., 1., 0., 1., 1., 1., 0., 0., 0., 0.,\n",
       "       0., 1., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 1., 0., 1., 0., 0.,\n",
       "       0., 1., 1., 0., 1., 1., 1., 1., 0., 0., 0., 0., 1., 0., 0., 0., 1.,\n",
       "       1., 0., 0., 1., 1., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_train[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([0.03162497, 0.01650003, 0.03650001, 0.0205    , 0.05875   ,\n",
       "        0.03299999, 0.08374998, 0.09124997, 0.09175   , 0.080625  ]),\n",
       " 'mean_score_time': array([0.00050002, 0.        , 0.        , 0.00212502, 0.00012499,\n",
       "        0.        , 0.00062501, 0.00024998, 0.00024998, 0.00199997]),\n",
       " 'mean_test_score': array([0.83654849, 0.50130344, 0.84019812, 0.50130344, 0.84280501,\n",
       "        0.77815433, 0.83811262, 0.83889468, 0.83785193, 0.83785193]),\n",
       " 'mean_train_score': array([0.86082956, 0.50130344, 0.86552187, 0.50130344, 0.86939525,\n",
       "        0.78426209, 0.86906007, 0.86924625, 0.86902285, 0.8689856 ]),\n",
       " 'param_C': masked_array(data=[0.0001, 0.0001, 0.001, 0.001, 0.01, 0.01, 1, 1, 10, 10],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_penalty': masked_array(data=['l2', 'l1', 'l2', 'l1', 'l2', 'l1', 'l2', 'l1', 'l2',\n",
       "                    'l1'],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'params': [{'C': 0.0001, 'penalty': 'l2'},\n",
       "  {'C': 0.0001, 'penalty': 'l1'},\n",
       "  {'C': 0.001, 'penalty': 'l2'},\n",
       "  {'C': 0.001, 'penalty': 'l1'},\n",
       "  {'C': 0.01, 'penalty': 'l2'},\n",
       "  {'C': 0.01, 'penalty': 'l1'},\n",
       "  {'C': 1, 'penalty': 'l2'},\n",
       "  {'C': 1, 'penalty': 'l1'},\n",
       "  {'C': 10, 'penalty': 'l2'},\n",
       "  {'C': 10, 'penalty': 'l1'}],\n",
       " 'rank_test_score': array([7, 9, 2, 9, 1, 8, 4, 3, 5, 5]),\n",
       " 'split0_test_score': array([0.84407484, 0.5010395 , 0.84407484, 0.5010395 , 0.84615385,\n",
       "        0.77962578, 0.83783784, 0.83991684, 0.83575884, 0.83575884]),\n",
       " 'split0_train_score': array([0.86020864, 0.50134128, 0.86467958, 0.50134128, 0.87153502,\n",
       "        0.78301043, 0.86974665, 0.86974665, 0.86974665, 0.86974665]),\n",
       " 'split1_test_score': array([0.82083333, 0.50208333, 0.82708333, 0.50208333, 0.82916667,\n",
       "        0.77291667, 0.82291667, 0.82083333, 0.82291667, 0.82291667]),\n",
       " 'split1_train_score': array([0.85816448, 0.5011919 , 0.86293206, 0.5011919 , 0.86948749,\n",
       "        0.78963051, 0.8715733 , 0.87127533, 0.87187128, 0.8715733 ]),\n",
       " 'split2_test_score': array([0.83333333, 0.50208333, 0.84166667, 0.50208333, 0.85      ,\n",
       "        0.79375   , 0.84166667, 0.84166667, 0.84166667, 0.84166667]),\n",
       " 'split2_train_score': array([0.85995232, 0.5011919 , 0.86293206, 0.5011919 , 0.86620977,\n",
       "        0.78694875, 0.86620977, 0.86650775, 0.86620977, 0.86620977]),\n",
       " 'split3_test_score': array([0.85386221, 0.50104384, 0.85803758, 0.50104384, 0.86430063,\n",
       "        0.76617954, 0.8559499 , 0.8559499 , 0.8559499 , 0.8559499 ]),\n",
       " 'split3_train_score': array([0.85850462, 0.50134048, 0.86446232, 0.50134048, 0.86654751,\n",
       "        0.78611856, 0.86595174, 0.86624963, 0.86535597, 0.86505809]),\n",
       " 'split4_test_score': array([0.81002088, 0.50104384, 0.81837161, 0.50104384, 0.81837161,\n",
       "        0.75782881, 0.82045929, 0.82254697, 0.82045929, 0.82045929]),\n",
       " 'split4_train_score': array([0.8668454 , 0.50134048, 0.87429252, 0.50134048, 0.87399464,\n",
       "        0.78582067, 0.87429252, 0.87607983, 0.87459041, 0.87459041]),\n",
       " 'split5_test_score': array([0.84968685, 0.50104384, 0.85177453, 0.50104384, 0.85177453,\n",
       "        0.78496868, 0.83924843, 0.84551148, 0.83924843, 0.83924843]),\n",
       " 'split5_train_score': array([0.86178135, 0.50134048, 0.86773905, 0.50134048, 0.86863271,\n",
       "        0.7813524 , 0.86744117, 0.86803694, 0.86773905, 0.86773905]),\n",
       " 'split6_test_score': array([0.84759916, 0.50104384, 0.85177453, 0.50104384, 0.84759916,\n",
       "        0.77035491, 0.84551148, 0.84551148, 0.84551148, 0.84551148]),\n",
       " 'split6_train_score': array([0.86356866, 0.50134048, 0.86505809, 0.50134048, 0.87012213,\n",
       "        0.78343759, 0.87042002, 0.86922848, 0.87012213, 0.86982425]),\n",
       " 'split7_test_score': array([0.83298539, 0.50104384, 0.82881002, 0.50104384, 0.83507307,\n",
       "        0.79958246, 0.84133612, 0.83924843, 0.84133612, 0.84133612]),\n",
       " 'split7_train_score': array([0.85761096, 0.50134048, 0.86207924, 0.50134048, 0.86863271,\n",
       "        0.77777778, 0.8668454 , 0.8668454 , 0.86654751, 0.86714328]),\n",
       " 'std_fit_time': array([0.00499842, 0.00680067, 0.00698206, 0.00692817, 0.00818156,\n",
       "        0.00696427, 0.0110085 , 0.01656618, 0.01324528, 0.0091779 ]),\n",
       " 'std_score_time': array([0.00050002, 0.        , 0.        , 0.00488455, 0.00033069,\n",
       "        0.        , 0.000696  , 0.00043298, 0.00043298, 0.00492439]),\n",
       " 'std_test_score': array([0.01424753, 0.00045059, 0.01313643, 0.00045059, 0.01355667,\n",
       "        0.01320462, 0.01083619, 0.01107043, 0.01086464, 0.01086464]),\n",
       " 'std_train_score': array([2.92876570e-03, 6.43984439e-05, 3.69380188e-03, 6.43984439e-05,\n",
       "        2.38981359e-03, 3.44007277e-03, 2.76873733e-03, 3.05719232e-03,\n",
       "        2.95758250e-03, 2.90411723e-03])}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search_logregr.cv_results_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train SVM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
       "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
       "     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
       "     verbose=0)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear_svm = LinearSVC()\n",
    "linear_svm.fit(features_train, labels_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train score: 0.862095933264\n",
      "Test score: 0.844376899696\n",
      "Diff score: 0.0177190335678\n"
     ]
    }
   ],
   "source": [
    "print(\"Train score: {}\".format(linear_svm.score(features_train, labels_train)))\n",
    "print(\"Test score: {}\".format(linear_svm.score(features_test, labels_test)))\n",
    "print(\"Diff score: {}\".format(\n",
    "    linear_svm.score(features_train, labels_train) - linear_svm.score(features_test, labels_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The difference between the train score and test score is relatively high and the train score is very high. That means high variance and/or imbalanced data between the training set and the test set. So we need to perform k-Fold coross validation with stratification and grid search to to evaluate the variance and the best parameters for this algorithm with the concrete amount of training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tune the Linear SVM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold_linear_cv = StratifiedKFold(n_splits=8) # train with 8\n",
    "params_linear_cv = {'C': [0.0001, 0.001, 0.01], 'penalty': ['l2']} # 0.01 is with smaller variance\n",
    "grid_search_linear_svm = GridSearchCV(\n",
    "    linear_svm, param_grid=params_linear_cv, cv=kfold_linear_cv, return_train_score=True)\n",
    "\n",
    "start = time.time()\n",
    "grid_search_linear_svm.fit(features_train, labels_train)\n",
    "end = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GridSearchCV took {1.94} seconds for 3 candidate parameter settings.\n"
     ]
    }
   ],
   "source": [
    "print(\"GridSearchCV took {%.2f} seconds for %d candidate parameter settings.\"\n",
    "      % (end - start, len(grid_search_linear_svm.cv_results_['params'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8441084462982273"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search_linear_svm.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 0.001, 'penalty': 'l2'}"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search_linear_svm.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid_search_linear_svm.grid_scores_\n",
    "# grid_search_linear_svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([0.02700001, 0.03712499, 0.15799999]),\n",
       " 'mean_score_time': array([0.0025    , 0.00012499, 0.00074998]),\n",
       " 'mean_test_score': array([0.83967675, 0.84410845, 0.84332638]),\n",
       " 'mean_train_score': array([0.86548456, 0.86809181, 0.86935797]),\n",
       " 'param_C': masked_array(data=[0.0001, 0.001, 0.01],\n",
       "              mask=[False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_penalty': masked_array(data=['l2', 'l2', 'l2'],\n",
       "              mask=[False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'params': [{'C': 0.0001, 'penalty': 'l2'},\n",
       "  {'C': 0.001, 'penalty': 'l2'},\n",
       "  {'C': 0.01, 'penalty': 'l2'}],\n",
       " 'rank_test_score': array([3, 1, 2]),\n",
       " 'split0_test_score': array([0.84407484, 0.85031185, 0.84823285]),\n",
       " 'split0_train_score': array([0.86348733, 0.87034277, 0.87034277]),\n",
       " 'split1_test_score': array([0.825     , 0.82916667, 0.825     ]),\n",
       " 'split1_train_score': array([0.86293206, 0.86769964, 0.87038141]),\n",
       " 'split2_test_score': array([0.8375    , 0.84791667, 0.84583333]),\n",
       " 'split2_train_score': array([0.86352801, 0.86531585, 0.86650775]),\n",
       " 'split3_test_score': array([0.85803758, 0.87056367, 0.86847599]),\n",
       " 'split3_train_score': array([0.86505809, 0.86446232, 0.86505809]),\n",
       " 'split4_test_score': array([0.81837161, 0.82254697, 0.82463466]),\n",
       " 'split4_train_score': array([0.87339887, 0.8728031 , 0.87697349]),\n",
       " 'split5_test_score': array([0.85386221, 0.8434238 , 0.84759916]),\n",
       " 'split5_train_score': array([0.86744117, 0.86773905, 0.8668454 ]),\n",
       " 'split6_test_score': array([0.84759916, 0.84551148, 0.8434238 ]),\n",
       " 'split6_train_score': array([0.86624963, 0.87042002, 0.8707179 ]),\n",
       " 'split7_test_score': array([0.83298539, 0.8434238 , 0.8434238 ]),\n",
       " 'split7_train_score': array([0.86178135, 0.86595174, 0.86803694]),\n",
       " 'std_fit_time': array([0.00788985, 0.00453975, 0.01595313]),\n",
       " 'std_score_time': array([0.00512349, 0.00033069, 0.000433  ]),\n",
       " 'std_test_score': array([0.01293458, 0.01343072, 0.013066  ]),\n",
       " 'std_train_score': array([0.00345105, 0.00270233, 0.00347372])}"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search_linear_svm.cv_results_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gaussian SVM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.49544072948328266"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm = SVC(C=0.0001, gamma=2)\n",
    "svm.fit(features_train, labels_train)\n",
    "svm.score(features_test, labels_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest Clasifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score: 0.750151975684\n",
      "f1_score: 0.749237339841\n"
     ]
    }
   ],
   "source": [
    "forest = RandomForestClassifier(n_estimators=1500, max_depth=2)\n",
    "forest.fit(features_train, labels_train)\n",
    "print(\"score: {}\".format(forest.score(features_test, labels_test)))\n",
    "print(\"f1_score: {}\".format(f1_score(labels_test, forest.predict(features_test))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train score: 0.782846715328\n"
     ]
    }
   ],
   "source": [
    "print(\"train score: {}\".format(forest.score(features_train, labels_train)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adaptive Boost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AdaBoostClassifier(algorithm='SAMME.R',\n",
       "          base_estimator=DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=1,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
       "            splitter='best'),\n",
       "          learning_rate=1.0, n_estimators=100, random_state=None)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "basic_forest = DecisionTreeClassifier(max_depth=1)\n",
    "ada_boost = AdaBoostClassifier(basic_forest, n_estimators=100)\n",
    "ada_boost.fit(features_train, labels_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7939209726443769"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ada_boost.score(features_test, labels_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8493222106360793"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ada_boost.score(features_train, labels_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# data:       https://github.com/opencv/opencv/tree/master/data/haarcascades\n",
    "# faces, see: https://github.com/opencv/opencv/blob/master/data/haarcascades/haarcascade_frontalface_default.xml\n",
    "face_cascade = cv2.CascadeClassifier('./faces_data/face.xml')\n",
    "my_image = None\n",
    "\n",
    "size = 50\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "is_running = True\n",
    "while True:\n",
    "    _, frame = cap.read()\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    # looks for faces within the image\n",
    "    faces = face_cascade.detectMultiScale(gray, 1.3, 5)\n",
    "    w = 10 #30\n",
    "    h = 10 #50\n",
    "    for (x, y, width, height) in faces:\n",
    "        #print(\"x: {}, y: {}, width: {}, height: {}\".format(x, y, width, height))\n",
    "        # draws a rectangle around the detected face\n",
    "        cv2.rectangle(frame, (x - w, y - h), (x + width + w, y + height + h), (255, 0, 0), 5)\n",
    "        face_gray = gray[(y - h): y + height + h, (x - w): x + width + w]\n",
    "\n",
    "        im_height = face_gray.shape[0]\n",
    "        im_width = face_gray.shape[1]\n",
    "        if cv2.waitKey(1) == ord('q'):\n",
    "            #cv2.imwrite(\"aaa.jpg\", face_gray)\n",
    "            resized = cv2.resize(face_gray, dsize=RESIZE_SHAPE, interpolation=cv2.INTER_CUBIC)\n",
    "            my_image = resized\n",
    "            #cv2.imwrite(\"bbbq.jpg\", resized)\n",
    "            time.sleep(2)\n",
    "            #print((np.ravel(face_gray)))\n",
    "            is_running = False\n",
    "            break\n",
    "\n",
    "    cv2.imshow('frame', frame)\n",
    "    #print(frame.shape)\n",
    "\n",
    "    if not is_running:\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# my_image = cv2.imread('./faces_data/fem_n2.JPG', cv2.IMREAD_GRAYSCALE)\n",
    "# my_image = cv2.resize(my_image, dsize=RESIZE_SHAPE)\n",
    "# my_image.shape\n",
    "\n",
    "# my_image = cv2.imread('./faces_data/gray_3.jpeg', cv2.IMREAD_GRAYSCALE)\n",
    "# img.shape\n",
    "# img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 78  86  93  98 100  52  36  37  41  43  47  46  41  39  39  42  40  50\n",
      "  47  45  37  36  38  33  34  32  36  39  38  41  58  72 146 168 168 170\n",
      " 170 170 171 171  71  80  92  93  83  40  40  43  51  51  61  54  63  61\n",
      "  52  54  57  61  61  57  60  49  45  52  48  43  43  41  44  44  54  61\n",
      "  83 163 169 169 167 169 169 169  69  78  90  84  41  45  44  54  52  59\n",
      "  64  75  83  88  93  96 107 104  94 103]\n",
      "(1600,)\n"
     ]
    }
   ],
   "source": [
    "#cv2.imwrite('./fem_n3.jpg', my_image)\n",
    "img = my_image.ravel()\n",
    "print(img[:100])\n",
    "print(img.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #cv2.imwrite('./test_img.jpg', my_image)\n",
    "# row = extract_features('./faces_data/fem_5.jpeg', FEMALE_CLASS)\n",
    "# img = row[:RESIZE_SHAPE[0]*RESIZE_SHAPE[1]]\n",
    "# img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# img = cv2.imread('./faces_data/male_d1.jpg', 0)\n",
    "# img = row[:RESIZE_SHAPE[0]*RESIZE_SHAPE[1]]\n",
    "# img = np.ravel(img)\n",
    "# img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 200)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img = pca.transform([img])\n",
    "img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = scaler.transform(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.06066343, 0.93933657]])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "result_1 = grid_search_logregr.best_estimator_.predict_proba(img)\n",
    "result_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.49971947, 0.50028053]])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_2 = ada_boost.predict_proba(img)\n",
    "result_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.46679494, 0.53320506]])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_3 = forest.predict_proba(img)\n",
    "result_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_3 = linear_svm.predict(img)\n",
    "result_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_4 = grid_search_linear_svm.predict(img)\n",
    "result_4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.010397, 0.989603]])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logistic_regression.predict_proba(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
